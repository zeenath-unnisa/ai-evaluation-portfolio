# AI Evaluation Portfolio

This repository documents applied work in **AI evaluation and human-in-the-loop (HITL) judgment**.

The focus is on assessing AI outputs for quality, safety, ambiguity, and alignment — areas where human reasoning remains essential. The work here reflects evaluation-style thinking rather than model building, with an emphasis on clear judgment, written rationale, and improvement-oriented feedback.

## What this portfolio contains

- **Human-in-the-loop evaluation cases**  
  Structured analyses of AI responses, edge cases, and failure modes that require qualitative human judgment.

- **Evaluation playbook**  
  A concise framework outlining how evaluations are conducted, including criteria, judgment principles, and feedback structure.

## How to read this repository

Start with the **playbook** to understand the evaluation approach, then review individual **cases** to see how that framework is applied in practice.

This portfolio is intended for roles involving AI evaluation, annotation with reasoning, safety review, and model feedback workflows.

## Sources & References

This portfolio is informed by established research and practitioner resources on AI evaluation, safety, and human-in-the-loop systems. The cases emphasise qualitative judgment and failure modes that are difficult to capture through automated metrics or rule-based checks alone.

### Evaluation, Safety & Alignment
- OpenAI – *Model Evaluation & Safety Frameworks*  
  https://platform.openai.com/docs/evaluations

- Anthropic – *Constitutional AI & Safety Research*  
  https://www.anthropic.com/research

- DeepMind – *Responsible AI & Safety*  
  https://deepmind.google/discover/blog

### Human-in-the-Loop & Failure Modes
- NIST – *AI Risk Management Framework (AI RMF)*  
  https://www.nist.gov/itl/ai-risk-management-framework

- Microsoft – *Human-AI Interaction Guidelines*  
  https://www.microsoft.com/en-us/research/group/human-ai-interaction/

- Google – *People + AI Research (PAIR)*  
  https://pair.withgoogle.com/

### Real-World Model Evaluation & Benchmarking
- HELM (Stanford) – *Holistic Evaluation of Language Models (qualitative and quantitative evaluation)*  
  https://crfm.stanford.edu/helm/

- Papers with Code – *Evaluation Benchmarks*  
  https://paperswithcode.com/

These references inform evaluation criteria, ambiguity handling, and human judgment considerations reflected throughout the cases.
