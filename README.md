# AI Evaluation Portfolio

This repository documents applied work in **AI evaluation and human-in-the-loop (HITL) judgment**.

The focus is on assessing AI outputs for quality, safety, ambiguity, and alignment â€” areas where human reasoning remains essential. The work here reflects evaluation-style thinking rather than model building, with an emphasis on clear judgment, written rationale, and improvement-oriented feedback.

## What this portfolio contains

- **Human-in-the-loop evaluation cases**  
  Structured analyses of AI responses, edge cases, and failure modes.

- **Evaluation playbook**  
  A concise framework outlining how evaluations are conducted, including criteria, judgment principles, and feedback structure.

## How to read this repository

Start with the **playbook** to understand the evaluation approach, then review individual **cases** to see how that framework is applied in practice.

This portfolio is intended for roles involving AI evaluation, annotation with reasoning, safety review, and model feedback workflows.
